<a target="_blank" href="https://colab.research.google.com/github/JPeggysus/How_LLMs_Work/">
  <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/>
</a>

The jupyter notebook in this repo will walk you step-by-step through how LLMs turn a sequnence of text into a next-token probability distribution. This lesson in grounded by connecting all ideas to a tiny LLM.
At then end, youget the chance to train this LLM using sample data made from a 20 token vocabulary.
I wish i had this resource when I started learning about transformers and LLMs. I hope it helps you!
